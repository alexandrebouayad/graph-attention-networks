{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset_geo = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "loader = DataLoader(dataset_geo, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CoraDataset\n",
    "\n",
    "dataset = CoraDataset(\"data/cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "train_loss: 1.9468\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9434\n",
      "valid_accuracy: 0.0720\n",
      "test_loss: 1.9963\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "train_loss: 1.9588\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9882\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9605\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "train_loss: 1.9495\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9553\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9267\n",
      "test_accuracy: 0.3190\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "train_loss: 1.9474\n",
      "train_accuracy: 0.1500\n",
      "valid_loss: 1.9291\n",
      "valid_accuracy: 0.2980\n",
      "test_loss: 1.9224\n",
      "test_accuracy: 0.3190\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "train_loss: 1.9498\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9259\n",
      "valid_accuracy: 0.3160\n",
      "test_loss: 1.9308\n",
      "test_accuracy: 0.3190\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "train_loss: 1.9491\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9334\n",
      "valid_accuracy: 0.3160\n",
      "test_loss: 1.9546\n",
      "test_accuracy: 0.0640\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "train_loss: 1.9470\n",
      "train_accuracy: 0.1714\n",
      "valid_loss: 1.9533\n",
      "valid_accuracy: 0.1400\n",
      "test_loss: 1.9679\n",
      "test_accuracy: 0.1030\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "train_loss: 1.9519\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9659\n",
      "valid_accuracy: 0.1140\n",
      "test_loss: 1.9561\n",
      "test_accuracy: 0.1030\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "train_loss: 1.9466\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9540\n",
      "valid_accuracy: 0.1160\n",
      "test_loss: 1.9548\n",
      "test_accuracy: 0.1300\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "train_loss: 1.9462\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9529\n",
      "valid_accuracy: 0.1220\n",
      "test_loss: 1.9535\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 11\n",
      "------------------------------\n",
      "train_loss: 1.9470\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9509\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9488\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 12\n",
      "------------------------------\n",
      "train_loss: 1.9457\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9472\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9435\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 13\n",
      "------------------------------\n",
      "train_loss: 1.9434\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9406\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9408\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 14\n",
      "------------------------------\n",
      "train_loss: 1.9477\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9381\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9416\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 15\n",
      "------------------------------\n",
      "train_loss: 1.9470\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9381\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9473\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 16\n",
      "------------------------------\n",
      "train_loss: 1.9469\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9431\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9523\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 17\n",
      "------------------------------\n",
      "train_loss: 1.9425\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9460\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9612\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 18\n",
      "------------------------------\n",
      "train_loss: 1.9421\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9569\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9729\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 19\n",
      "------------------------------\n",
      "train_loss: 1.9299\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9630\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9851\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 20\n",
      "------------------------------\n",
      "train_loss: 1.9020\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9848\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9935\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 21\n",
      "------------------------------\n",
      "train_loss: 1.8784\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9889\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9891\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 22\n",
      "------------------------------\n",
      "train_loss: 1.8390\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9681\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9757\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 23\n",
      "------------------------------\n",
      "train_loss: 1.8414\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9519\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9496\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 24\n",
      "------------------------------\n",
      "train_loss: 1.8065\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9438\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9307\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 25\n",
      "------------------------------\n",
      "train_loss: 1.7788\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9350\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9226\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 26\n",
      "------------------------------\n",
      "train_loss: 1.9125\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9458\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9209\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 27\n",
      "------------------------------\n",
      "train_loss: 1.7920\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9421\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9201\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 28\n",
      "------------------------------\n",
      "train_loss: 1.8723\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9441\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9193\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 29\n",
      "------------------------------\n",
      "train_loss: 1.9320\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9459\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9215\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 30\n",
      "------------------------------\n",
      "train_loss: 1.8516\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9456\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9208\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 31\n",
      "------------------------------\n",
      "train_loss: 1.9263\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9459\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9173\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 32\n",
      "------------------------------\n",
      "train_loss: 1.9312\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9459\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9194\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 33\n",
      "------------------------------\n",
      "train_loss: 1.8351\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9428\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9149\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 34\n",
      "------------------------------\n",
      "train_loss: 1.8624\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9432\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9132\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 35\n",
      "------------------------------\n",
      "train_loss: 1.8804\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9455\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9213\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 36\n",
      "------------------------------\n",
      "train_loss: 1.8053\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9271\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9430\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 37\n",
      "------------------------------\n",
      "train_loss: 1.9315\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9459\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 1.9679\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 38\n",
      "------------------------------\n",
      "train_loss: 1.8597\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9340\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.0057\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 39\n",
      "------------------------------\n",
      "train_loss: 1.9216\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9447\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.0435\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 40\n",
      "------------------------------\n",
      "train_loss: 1.9071\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9424\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.0858\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 41\n",
      "------------------------------\n",
      "train_loss: 1.8503\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9391\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.1464\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 42\n",
      "------------------------------\n",
      "train_loss: 1.8878\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9439\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.2011\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 43\n",
      "------------------------------\n",
      "train_loss: 1.8604\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 2.2939\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.1976\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 44\n",
      "------------------------------\n",
      "train_loss: 1.9163\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9459\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.1839\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 45\n",
      "------------------------------\n",
      "train_loss: 1.9116\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9460\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.1685\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 46\n",
      "------------------------------\n",
      "train_loss: 1.8628\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 1.9429\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.2003\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 47\n",
      "------------------------------\n",
      "train_loss: 1.9094\n",
      "train_accuracy: 0.1429\n",
      "valid_loss: 2.2829\n",
      "valid_accuracy: 0.1620\n",
      "test_loss: 2.1810\n",
      "test_accuracy: 0.1490\n",
      "\n",
      "Epoch 48\n",
      "------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0v/kyjb3cqj1pz5t9zqznwj_8100000gn/T/ipykernel_44543/141171535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ml/graph-attention-networks/run.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         train_loop(\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/run.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, patience)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adjacency)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adjacency)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adjacency)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    295\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 297\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    298\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from run import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0v/kyjb3cqj1pz5t9zqznwj_8100000gn/T/ipykernel_44543/190186942.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, '\n",
      "\u001b[0;32m/var/folders/0v/kyjb3cqj1pz5t9zqznwj_8100000gn/T/ipykernel_44543/190186942.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/graph-attention-networks/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, adjacency)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "from model import GraphAttentionNetwork\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = CoraDataset(\"data/cora\", device=device)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "for features, labels, adjacency, train_mask, valid_mask, test_mask in dataloader:\n",
    "    break\n",
    "\n",
    "model = GraphAttentionNetwork(\n",
    "    in_features=dataset.n_features,\n",
    "    hidden_units=8,\n",
    "    n_classes=dataset.n_classes,\n",
    "    n_heads=[8, 1],\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.005,\n",
    "    weight_decay=0.0005,\n",
    ")\n",
    "\n",
    "def train(featrues, adjacency, train_mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(features, adjacency)\n",
    "    loss = F.cross_entropy(out[train_mask], labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(features, adjacency, valid_mask, test_mask):\n",
    "    model.eval()\n",
    "    out, accs = model(features, adjacency), []\n",
    "    for mask in train_mask, valid_mask, test_mask:\n",
    "        acc = float((out[mask].argmax(-1) == labels[mask]).sum() / mask.sum())\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train(features, adjacency, train_mask)\n",
    "    train_acc, val_acc, test_acc = test(features, adjacency, valid_mask, test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val: {val_acc:.4f}, '\n",
    "          f'Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa8396e1be5a4bc07acb27fc09bf28ed9e116af37c9ee75084c214c57090453e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
